{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Setup The Project "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-12-19T21:37:25.089399Z","iopub.status.busy":"2024-12-19T21:37:25.088931Z","iopub.status.idle":"2024-12-19T21:37:25.099207Z","shell.execute_reply":"2024-12-19T21:37:25.097300Z","shell.execute_reply.started":"2024-12-19T21:37:25.089361Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from pathlib import Path\n","import time\n","import psutil\n","from typing import Dict, List, Tuple, Optional, Union\n","from sklearn.model_selection import KFold, train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import mean_squared_log_error, mean_squared_error, mean_absolute_error, r2_score\n","from sklearn.base import BaseEstimator, TransformerMixin\n","import xgboost as xgb\n","import lightgbm as lgb\n","import catboost as cb\n","import warnings\n","\n","RANDOM_STATE = 42\n","warnings.filterwarnings('ignore')\n","np.random.seed(RANDOM_STATE)\n","\n","print(\"Setup completed successfully\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T21:37:25.514366Z","iopub.status.busy":"2024-12-19T21:37:25.513962Z","iopub.status.idle":"2024-12-19T21:37:25.521858Z","shell.execute_reply":"2024-12-19T21:37:25.520610Z","shell.execute_reply.started":"2024-12-19T21:37:25.514332Z"},"trusted":true},"outputs":[],"source":["import os\n","for dirname, _, filenames in os.walk('/kaggle/input/playground-series-s4e12'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"markdown","metadata":{},"source":["## Pipline Monitor\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T21:37:26.969853Z","iopub.status.busy":"2024-12-19T21:37:26.969469Z","iopub.status.idle":"2024-12-19T21:37:26.978232Z","shell.execute_reply":"2024-12-19T21:37:26.976632Z","shell.execute_reply.started":"2024-12-19T21:37:26.969823Z"},"trusted":true},"outputs":[],"source":["class PipelineMonitor:\n","    def __init__(self):\n","        self.start_time = time.time()\n","        self.checkpoints = {}\n","\n","    def checkpoint(self, name: str) -> None:\n","        \"\"\"Record time taken since last checkpoint\"\"\"\n","        current_time = time.time()\n","        time_taken = current_time - self.start_time\n","        self.checkpoints[name] = time_taken\n","        self.start_time = current_time\n","\n","    def get_memory_usage(self) -> float:\n","        \"\"\"Get current memory usage in MB\"\"\"\n","        process = psutil.Process()\n","        return process.memory_info().rss / 1024 / 1024\n","\n","# Initialize pipeline monitor\n","monitor = PipelineMonitor()"]},{"cell_type":"markdown","metadata":{},"source":["## Collecting Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T21:37:30.932045Z","iopub.status.busy":"2024-12-19T21:37:30.931551Z","iopub.status.idle":"2024-12-19T21:37:41.401910Z","shell.execute_reply":"2024-12-19T21:37:41.400388Z","shell.execute_reply.started":"2024-12-19T21:37:30.932007Z"},"trusted":true},"outputs":[],"source":["print(\"Loading data...\")\n","data_dir = Path(\"data\")\n","train_path = data_dir / \"train.csv\"\n","test_path = data_dir / \"test.csv\"\n","\n","\n","try:\n","    train_df = pd.read_csv(train_path)\n","    test_df = pd.read_csv(test_path)\n","    print(\"Data loaded successfully!\")\n","except FileNotFoundError as e:\n","    print(f\"Error: Data files not found. Please ensure the data files are in the 'data' directory.\\nError: {e}\")\n","    raise\n","\n","monitor.checkpoint(\"Collecting Data Complete\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exploratory Data Analysis (EDA)"]},{"cell_type":"markdown","metadata":{},"source":["### Basic Information"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T21:37:41.404696Z","iopub.status.busy":"2024-12-19T21:37:41.404186Z","iopub.status.idle":"2024-12-19T21:37:41.412391Z","shell.execute_reply":"2024-12-19T21:37:41.411353Z","shell.execute_reply.started":"2024-12-19T21:37:41.404642Z"},"trusted":true},"outputs":[],"source":["print(\"\\n1. Basic Information:\")\n","print(\"-\" * 50)\n","print(\"\\nDataset Shape:\", train_df.shape)\n","print(\"\\nFeature Types:\")\n","print(train_df.dtypes)"]},{"cell_type":"markdown","metadata":{},"source":["### Missing Values Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T21:37:41.414149Z","iopub.status.busy":"2024-12-19T21:37:41.413828Z","iopub.status.idle":"2024-12-19T21:37:42.102031Z","shell.execute_reply":"2024-12-19T21:37:42.100670Z","shell.execute_reply.started":"2024-12-19T21:37:41.414120Z"},"trusted":true},"outputs":[],"source":["print(\"\\n2. Missing Values Analysis:\")\n","print(\"-\" * 50)\n","missing = train_df.isnull().sum()\n","missing_pct = (missing / len(train_df)) * 100\n","missing_info = pd.DataFrame({\n","    'Missing Values': missing,\n","    'Percentage': missing_pct\n","})\n","\n","print(missing_info[missing_info['Missing Values'] > 0])"]},{"cell_type":"markdown","metadata":{},"source":["### Numerical Features Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T21:37:42.104941Z","iopub.status.busy":"2024-12-19T21:37:42.104463Z","iopub.status.idle":"2024-12-19T21:37:42.850458Z","shell.execute_reply":"2024-12-19T21:37:42.849209Z","shell.execute_reply.started":"2024-12-19T21:37:42.104891Z"},"trusted":true},"outputs":[],"source":["print(\"\\n3. Numerical Features Analysis:\")\n","print(\"-\" * 50)\n","numeric_cols = train_df.select_dtypes(include=['int64', 'float64']).columns\n","numeric_summary = train_df[numeric_cols].describe()\n","print(numeric_summary)"]},{"cell_type":"markdown","metadata":{},"source":["### Categorical Features Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T21:37:42.852221Z","iopub.status.busy":"2024-12-19T21:37:42.851822Z","iopub.status.idle":"2024-12-19T21:37:45.442535Z","shell.execute_reply":"2024-12-19T21:37:45.440987Z","shell.execute_reply.started":"2024-12-19T21:37:42.852184Z"},"trusted":true},"outputs":[],"source":["print(\"\\n4. Categorical Features Analysis:\")\n","print(\"-\" * 50)\n","categorical_cols = train_df.select_dtypes(include=['object']).columns\n","for col in categorical_cols:\n","    print(f\"\\nUnique values in {col}:\", train_df[col].nunique())\n","    print(\"\\nTop 5 categories:\")\n","    print(train_df[col].value_counts().head())"]},{"cell_type":"markdown","metadata":{},"source":["### Visualizations"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T21:38:05.345385Z","iopub.status.busy":"2024-12-19T21:38:05.344346Z","iopub.status.idle":"2024-12-19T21:38:59.116018Z","shell.execute_reply":"2024-12-19T21:38:59.114960Z","shell.execute_reply.started":"2024-12-19T21:38:05.345340Z"},"trusted":true},"outputs":[],"source":["# Distribution plots for numerical features\n","plt.figure(figsize=(15, 5 * ((len(numeric_cols) + 2) // 3)))\n","for i, col in enumerate(numeric_cols, 1):\n","    plt.subplot((len(numeric_cols) + 2) // 3, 3, i)\n","    sns.histplot(train_df[col], kde=True)\n","    plt.title(f'Distribution of {col}')\n","    plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n","\n","for i, col in enumerate(numeric_cols, 1):\n","    plt.subplot((len(numeric_cols) + 2) // 3, 3, i)\n","    sns.boxplot(y=df[col])\n","    plt.title(f'Box Plot of {col}')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T21:40:41.415732Z","iopub.status.busy":"2024-12-19T21:40:41.415217Z","iopub.status.idle":"2024-12-19T21:40:42.545725Z","shell.execute_reply":"2024-12-19T21:40:42.544514Z","shell.execute_reply.started":"2024-12-19T21:40:41.415691Z"},"trusted":true},"outputs":[],"source":["# Correlation analysis\n","plt.figure(figsize=(12, 8))\n","correlation_matrix = train_df[numeric_cols].corr()\n","sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n","plt.title('Correlation Matrix of Numerical Features')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T21:46:04.015365Z","iopub.status.busy":"2024-12-19T21:46:04.014944Z","iopub.status.idle":"2024-12-19T21:46:06.341841Z","shell.execute_reply":"2024-12-19T21:46:06.340745Z","shell.execute_reply.started":"2024-12-19T21:46:04.015327Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(15, 5 * ((len(numeric_cols) + 2) // 3)))\n","for i, col in enumerate(numeric_cols, 1):\n","    plt.subplot((len(numeric_cols) + 2) // 3, 3, i)\n","    sns.boxplot(y=train_df[col])\n","    plt.title(f'Box Plot of {col}')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T21:43:40.516852Z","iopub.status.busy":"2024-12-19T21:43:40.515734Z","iopub.status.idle":"2024-12-19T21:44:21.061764Z","shell.execute_reply":"2024-12-19T21:44:21.060717Z","shell.execute_reply.started":"2024-12-19T21:43:40.516804Z"},"trusted":true},"outputs":[],"source":["# Target variable analysis\n","target_col = 'Premium Amount'\n","\n","if target_col in train_df.columns:\n","    plt.figure(figsize=(12, 5))\n","\n","    # Target distribution\n","    plt.subplot(1, 2, 1)\n","    sns.histplot(train_df[target_col], kde=True)\n","    plt.title('Distribution of Premium Amount')\n","\n","    # Log-transformed target distribution\n","    plt.subplot(1, 2, 2)\n","    sns.histplot(np.log1p(train_df[target_col]), kde=True)\n","    plt.title('Distribution of Log-transformed Premium Amount')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Relationship between target and numerical features\n","    plt.figure(figsize=(15, 5 * ((len(numeric_cols) - 1 + 2) // 3)))\n","    for i, col in enumerate([c for c in numeric_cols if c != target_col], 1):\n","        plt.subplot((len(numeric_cols) - 1 + 2) // 3, 3, i)\n","        plt.scatter(train_df[col], train_df[target_col], alpha=0.5)\n","        plt.xlabel(col)\n","        plt.ylabel(target_col)\n","        plt.title(f'{col} vs {target_col}')\n","    plt.tight_layout()\n","    plt.show()\n","\n","monitor.checkpoint(\"EDA Complete\")"]},{"cell_type":"markdown","metadata":{},"source":["## Data Validation and Cleaning"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T21:47:24.421206Z","iopub.status.busy":"2024-12-19T21:47:24.420813Z","iopub.status.idle":"2024-12-19T21:47:24.430910Z","shell.execute_reply":"2024-12-19T21:47:24.429691Z","shell.execute_reply.started":"2024-12-19T21:47:24.421174Z"},"trusted":true},"outputs":[],"source":["def validate_data(df: pd.DataFrame) -> Tuple[bool, Dict[str, bool]]:\n","    checks = {\n","        'missing_values': df.isnull().sum().sum() == 0,\n","        'negative_values': (df.select_dtypes(include=['int64', 'float64']) >= 0).all().all(),\n","        'duplicates': df.duplicated().sum() == 0,\n","    }\n","    return all(checks.values()), checks\n","\n","def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Comprehensive data preprocessing pipeline\n","\n","    Parameters:\n","        df: Input DataFrame\n","\n","    Returns:\n","        Preprocessed DataFrame\n","    \"\"\"\n","    df = df.copy()\n","\n","    # Handle missing values\n","    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n","    categorical_cols = df.select_dtypes(include=['object']).columns\n","\n","    numeric_imputer = SimpleImputer(strategy='median')\n","    categorical_imputer = SimpleImputer(strategy='most_frequent')\n","\n","    df[numeric_cols] = numeric_imputer.fit_transform(df[numeric_cols])\n","    df[categorical_cols] = categorical_imputer.fit_transform(df[categorical_cols])\n","\n","    # Handle outliers using IQR method\n","    for col in numeric_cols:\n","        Q1 = df[col].quantile(0.25)\n","        Q3 = df[col].quantile(0.75)\n","        IQR = Q3 - Q1\n","        df[col] = df[col].clip(lower=Q1 - 1.5*IQR, upper=Q3 + 1.5*IQR)\n","\n","\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Engineering"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T21:47:47.570118Z","iopub.status.busy":"2024-12-19T21:47:47.569625Z","iopub.status.idle":"2024-12-19T21:47:47.598010Z","shell.execute_reply":"2024-12-19T21:47:47.596557Z","shell.execute_reply.started":"2024-12-19T21:47:47.570080Z"},"trusted":true},"outputs":[],"source":["class FeatureEngineer(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        self.label_encoders = {}\n","        self.scaler = RobustScaler()\n","        self.standard_scaler = StandardScaler()\n","        self.feature_names = None\n","        self.numeric_features = None\n","        self.categorical_features = None\n","\n","    def standardize_column_names(self, df: pd.DataFrame) -> pd.DataFrame:\n","        \"\"\"Standardizes column names by replacing spaces with underscores and converting to lowercase.\"\"\"\n","        df.columns = df.columns.str.replace(' ', '_').str.replace('/', '_').str.lower()\n","        return df\n","\n","    def transform_dates(self, df: pd.DataFrame) -> pd.DataFrame:\n","        \"\"\"Extracts date-related features and drops the original date column.\"\"\"\n","        df = df.copy()\n","        if 'policy_start_date' in df.columns:\n","            df['policy_start_date'] = pd.to_datetime(df['policy_start_date'], errors='coerce')\n","            df['policy_start_year'] = df['policy_start_date'].dt.year\n","            df['policy_start_month'] = df['policy_start_date'].dt.month\n","            df['policy_start_quarter'] = df['policy_start_date'].dt.quarter\n","            df.drop('policy_start_date', axis=1, inplace=True)\n","        return df\n","\n","    def create_interactions(self, df: pd.DataFrame) -> pd.DataFrame:\n","        \"\"\"Generates basic and advanced interaction features.\"\"\"\n","        df = df.copy()\n","        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n","\n","        # Store numeric features for later use\n","        self.numeric_features = [col for col in numeric_cols if col not in ['id', 'premium_amount']]\n","\n","        # Basic ratio features\n","        if 'annual_income' in df.columns and 'age' in df.columns:\n","            df['age_income_ratio'] = df['age'] / df['annual_income'].clip(lower=1)\n","\n","        if 'annual_income' in df.columns and 'number_of_dependents' in df.columns:\n","            df['income_per_dependent'] = df['annual_income'] / (df['number_of_dependents'].clip(lower=0) + 1)\n","\n","        if 'health_score' in df.columns and 'age' in df.columns:\n","            df['health_age_ratio'] = df['health_score'] / df['age'].clip(lower=1)\n","\n","        # Risk-based features\n","        if all(col in df.columns for col in ['health_score', 'credit_score', 'age']):\n","            df['risk_score'] = (df['health_score'] * df['credit_score']) / df['age'].clip(lower=1)\n","\n","        if all(col in df.columns for col in ['previous_claims', 'insurance_duration']):\n","            df['claims_duration_ratio'] = df['previous_claims'] / df['insurance_duration'].clip(lower=1)\n","\n","        # Demographic features\n","        if 'number_of_dependents' in df.columns and 'annual_income' in df.columns:\n","            df['dependent_burden'] = df['number_of_dependents'] / df['annual_income'].clip(lower=1)\n","\n","        if 'age' in df.columns and 'health_score' in df.columns:\n","            df['age_risk_factor'] = df['age'] * (1 / df['health_score'].clip(lower=1))\n","\n","        # Insurance history features\n","        if 'previous_claims' in df.columns and 'insurance_duration' in df.columns:\n","            df['claims_frequency'] = df['previous_claims'] / df['insurance_duration'].clip(lower=1)\n","\n","        # Polynomial features for key numeric columns\n","        for col in ['age', 'health_score', 'credit_score']:\n","            if col in df.columns:\n","                df[f'{col}_squared'] = df[col] ** 2\n","\n","        return df\n","\n","    def encode_categoricals(self, df: pd.DataFrame, is_training: bool = True) -> pd.DataFrame:\n","        \"\"\"Encodes categorical features with Label Encoding.\"\"\"\n","        df = df.copy()\n","        categorical_cols = df.select_dtypes(include=['object']).columns\n","\n","        # Store categorical features for later use\n","        if is_training:\n","            self.categorical_features = list(categorical_cols)\n","\n","        for col in categorical_cols:\n","            df[col] = df[col].fillna('Unknown')\n","            if is_training:\n","                self.label_encoders[col] = LabelEncoder()\n","                df[col] = self.label_encoders[col].fit_transform(df[col])\n","            else:\n","                if col in self.label_encoders:\n","                    # Get unique categories and create a mapping dictionary\n","                    known_categories = set(self.label_encoders[col].classes_)\n","                    unique_vals = df[col].unique()\n","                    val_map = {val: (\n","                        self.label_encoders[col].transform([val])[0] if val in known_categories else -1\n","                    ) for val in unique_vals}\n","                    # Use replace which is much faster than map\n","                    df[col] = df[col].replace(val_map)\n","        return df\n","\n","    def scale_features(self, df: pd.DataFrame, is_training: bool = True) -> pd.DataFrame:\n","        \"\"\"Scales numerical features using RobustScaler.\"\"\"\n","        df = df.copy()\n","        if not self.numeric_features:\n","            self.numeric_features = df.select_dtypes(include=['int64', 'float64']).columns\n","            self.numeric_features = [col for col in self.numeric_features if col not in ['id', 'premium_amount']]\n","\n","        if self.numeric_features:\n","            if is_training:\n","                df[self.numeric_features] = self.scaler.fit_transform(df[self.numeric_features].fillna(0))\n","            else:\n","                df[self.numeric_features] = self.scaler.transform(df[self.numeric_features].fillna(0))\n","        return df\n","\n","    def transform(self, df: pd.DataFrame, is_training: bool = True) -> pd.DataFrame:\n","        \"\"\"Applies all transformations in sequence.\"\"\"\n","        df = self.standardize_column_names(df)\n","        df = self.transform_dates(df)\n","        df = self.create_interactions(df)\n","        df = self.encode_categoricals(df, is_training)\n","        df = self.scale_features(df, is_training)\n","\n","        # Store feature names after all transformations\n","        self.feature_names = list(df.columns)\n","        return df"]},{"cell_type":"markdown","metadata":{},"source":["## Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T21:48:14.201614Z","iopub.status.busy":"2024-12-19T21:48:14.201144Z","iopub.status.idle":"2024-12-19T21:48:14.231998Z","shell.execute_reply":"2024-12-19T21:48:14.230576Z","shell.execute_reply.started":"2024-12-19T21:48:14.201554Z"},"trusted":true},"outputs":[],"source":["class ModelTrainer:\n","    def __init__(self, random_state=42):\n","        self.random_state = random_state\n","        self.models = {}\n","        self.y_true_all_folds = []\n","        self.y_pred_all_folds = []\n","        self.best_model = None\n","        self.feature_importance = None\n","\n","    def train_xgboost(self, X_train, y_train, X_val=None, y_val=None):\n","        \"\"\"Train XGBoost model with early stopping if validation data is provided.\"\"\"\n","        params = {\n","            'objective': 'reg:squarederror',\n","            'eval_metric': 'rmse',\n","            'max_depth': 8,\n","            'learning_rate': 0.05,\n","            'n_estimators': 1000,\n","            'min_child_weight': 3,\n","            'subsample': 0.8,\n","            'colsample_bytree': 0.8,\n","            'random_state': self.random_state,\n","            'early_stopping_rounds': 50,\n","            'verbose': False\n","        }\n","\n","        model = xgb.XGBRegressor(**params)\n","\n","        if X_val is not None and y_val is not None:\n","            model.fit(\n","                X_train, y_train,\n","                eval_set=[(X_train, y_train), (X_val, y_val)]\n","            )\n","        else:\n","            model.fit(X_train, y_train)\n","\n","        return model\n","\n","    def train_lightgbm(self, X_train, y_train, X_val=None, y_val=None):\n","        \"\"\"Train LightGBM model with early stopping if validation data is provided.\"\"\"\n","        params = {\n","            'objective': 'regression',\n","            'metric': 'rmse',\n","            'num_leaves': 31,\n","            'learning_rate': 0.05,\n","            'feature_fraction': 0.8,\n","            'bagging_fraction': 0.8,\n","            'bagging_freq': 5,\n","            'random_state': self.random_state,\n","            'n_estimators': 1000,\n","            'verbose': -1\n","        }\n","\n","        model = lgb.LGBMRegressor(**params)\n","\n","        if X_val is not None and y_val is not None:\n","            callbacks = [lgb.early_stopping(stopping_rounds=50)]\n","            model.fit(\n","                X_train, y_train,\n","                eval_set=[(X_val, y_val)],\n","                callbacks=callbacks\n","            )\n","        else:\n","            model.fit(X_train, y_train)\n","\n","        return model\n","\n","    def train_catboost(self, X_train, y_train, X_val=None, y_val=None):\n","        \"\"\"Train CatBoost model with early stopping if validation data is provided.\"\"\"\n","        params = {\n","            'loss_function': 'RMSE',\n","            'eval_metric': 'RMSE',\n","            'learning_rate': 0.05,\n","            'depth': 6,\n","            'iterations': 1000,\n","            'random_seed': self.random_state,\n","            'verbose': False\n","        }\n","\n","        model = cb.CatBoostRegressor(**params)\n","\n","        if X_val is not None and y_val is not None:\n","            model.fit(\n","                X_train, y_train,\n","                eval_set=(X_val, y_val),\n","                early_stopping_rounds=50,\n","                verbose=False\n","            )\n","        else:\n","            model.fit(X_train, y_train)\n","\n","        return model\n","\n","    def train_with_kfold(self, X, y, n_splits=5):\n","        \"\"\"Train models using K-Fold cross validation.\"\"\"\n","        print(\"\\nStarting K-Fold Cross Validation Training...\")\n","        kf = KFold(n_splits=n_splits, shuffle=True, random_state=self.random_state)\n","\n","        fold_scores = {\n","            'xgboost': [],\n","            'lightgbm': [],\n","            'catboost': []\n","        }\n","\n","        model_predictions = {\n","            'xgboost': [],\n","            'lightgbm': [],\n","            'catboost': []\n","        }\n","\n","        for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n","            print(f\"\\nTraining Fold {fold}/{n_splits}\")\n","\n","            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n","            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n","\n","            # Train each model\n","            self.models[f'xgboost_fold_{fold}'] = self.train_xgboost(X_train, y_train, X_val, y_val)\n","            self.models[f'lightgbm_fold_{fold}'] = self.train_lightgbm(X_train, y_train, X_val, y_val)\n","            self.models[f'catboost_fold_{fold}'] = self.train_catboost(X_train, y_train, X_val, y_val)\n","\n","            # Make predictions\n","            y_pred_xgb = self.models[f'xgboost_fold_{fold}'].predict(X_val)\n","            y_pred_lgb = self.models[f'lightgbm_fold_{fold}'].predict(X_val)\n","            y_pred_cb = self.models[f'catboost_fold_{fold}'].predict(X_val)\n","\n","            # Store predictions\n","            model_predictions['xgboost'].append((fold, y_pred_xgb))\n","            model_predictions['lightgbm'].append((fold, y_pred_lgb))\n","            model_predictions['catboost'].append((fold, y_pred_cb))\n","\n","            # Calculate RMSLE for each model\n","            fold_scores['xgboost'].append(np.sqrt(mean_squared_log_error(y_val, y_pred_xgb)))\n","            fold_scores['lightgbm'].append(np.sqrt(mean_squared_log_error(y_val, y_pred_lgb)))\n","            fold_scores['catboost'].append(np.sqrt(mean_squared_log_error(y_val, y_pred_cb)))\n","\n","            # Store true values for this fold\n","            self.y_true_all_folds.extend(y_val)\n","\n","            # Store ensemble predictions for this fold\n","            ensemble_pred = (y_pred_xgb + y_pred_lgb + y_pred_cb) / 3\n","            self.y_pred_all_folds.extend(ensemble_pred)\n","\n","        # Print average scores\n","        print(\"\\nAverage RMSLE scores across folds:\")\n","        avg_scores = {}\n","        for model_name, scores in fold_scores.items():\n","            avg_score = np.mean(scores)\n","            std_score = np.std(scores)\n","            print(f\"{model_name}: {avg_score:.4f} (+/- {std_score:.4f})\")\n","            avg_scores[model_name] = avg_score\n","\n","        # Select best model type based on average performance\n","        best_model_type = min(avg_scores.items(), key=lambda x: x[1])[0]\n","\n","        # Find the best fold for the best model type\n","        best_fold_idx = np.argmin(fold_scores[best_model_type])\n","        best_fold = best_fold_idx + 1\n","\n","        # Set the best model\n","        self.best_model = self.models[f'{best_model_type}_fold_{best_fold}']\n","\n","        # Get feature importance from the best model\n","        if hasattr(self.best_model, 'feature_importances_'):\n","            self.feature_importance = self.best_model.feature_importances_\n","\n","        print(f\"\\nBest Model: {best_model_type} from fold {best_fold}\")\n","        return self.models\n","\n","    def get_y_true_and_pred(self):\n","        \"\"\"Get true and predicted values from all folds.\"\"\"\n","        return np.array(self.y_true_all_folds), np.array(self.y_pred_all_folds)\n","\n","    def evaluate_models(self):\n","        \"\"\"Evaluate all trained models using RMSLE metric.\"\"\"\n","        if not self.y_true_all_folds or not self.y_pred_all_folds:\n","            print(\"No predictions available. Please train the models first.\")\n","            return None\n","\n","        y_true = np.array(self.y_true_all_folds)\n","        y_pred = np.array(self.y_pred_all_folds)\n","\n","        metrics = {\n","            'RMSLE': np.sqrt(mean_squared_log_error(y_true, y_pred)),\n","            'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n","            'MAE': mean_absolute_error(y_true, y_pred),\n","            'R2': r2_score(y_true, y_pred)\n","        }\n","\n","        print(\"\\nModel Evaluation Metrics:\")\n","        for metric_name, value in metrics.items():\n","            print(f\"{metric_name}: {value:.4f}\")\n","\n","        return metrics\n","\n","    def predict(self, X):\n","        \"\"\"Make predictions using the best model.\"\"\"\n","        if self.best_model is None:\n","            raise ValueError(\"No model available. Please train the models first.\")\n","        return self.best_model.predict(X)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Training and Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T21:48:46.080541Z","iopub.status.busy":"2024-12-19T21:48:46.079527Z","iopub.status.idle":"2024-12-19T22:06:26.473838Z","shell.execute_reply":"2024-12-19T22:06:26.472629Z","shell.execute_reply.started":"2024-12-19T21:48:46.080495Z"},"trusted":true},"outputs":[],"source":["# Start the training pipeline\n","print(\"\\nStarting main training pipeline...\")\n","\n","# Initialize the feature engineer\n","fe = FeatureEngineer()\n","\n","# Prepare train and test data\n","print(\"Preparing train and test data...\")\n","X_train = train_df.drop(['Premium Amount', 'id'], axis=1)\n","y_train = np.log1p(train_df['Premium Amount'])  # Log transform target\n","X_test = test_df.drop(['id'], axis=1)\n","\n","# Apply feature transformations\n","print(\"Transforming features...\")\n","X_train_transformed = fe.transform(X_train, is_training=True)\n","X_test_transformed = fe.transform(X_test, is_training=False)\n","print(\"Features prepared!\")\n","\n","# Initialize the model trainer\n","trainer = ModelTrainer(random_state=42)\n","\n","# Train models with K-Fold Cross-Validation\n","print(\"Training models with K-Fold Cross-Validation...\")\n","try:\n","    models = trainer.train_with_kfold(X_train_transformed, y_train)\n","    print(\"Training completed!\")\n","\n","    # Evaluate models\n","    print(\"Evaluating models...\")\n","    evaluations = trainer.evaluate_models()\n","    print(f\"Model Evaluations: {evaluations}\")\n","\n","    # Make predictions with the best model\n","    print(\"Making predictions with the best model...\")\n","    if trainer.best_model is not None:\n","        test_predictions = trainer.predict(X_test_transformed)\n","\n","        # Transform predictions back from log space\n","        final_predictions = np.expm1(test_predictions)\n","\n","        # Prepare submission\n","        submission = pd.DataFrame({\n","            'id': test_df['id'],\n","            'Premium Amount': final_predictions\n","        })\n","\n","        # Save predictions\n","        output_path = 'submission.csv'\n","        submission.to_csv(output_path, index=False)\n","        print(f\"\\nPredictions saved to {output_path}\")\n","\n","    else:\n","        print(\"Error: Best model not selected properly during training.\")\n","except Exception as e:\n","    print(f\"Error during model training/prediction: {str(e)}\")\n","\n","monitor.checkpoint(\"Analysis Complete\")\n","\n","# Print final execution summary\n","print(\"\\nExecution Summary:\")\n","for checkpoint, time_taken in monitor.checkpoints.items():\n","    print(f\"{checkpoint}: {time_taken:.2f}s\")\n","print(f\"Final Memory Usage: {monitor.get_memory_usage():.2f} MB\")"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":10305135,"sourceId":84896,"sourceType":"competition"}],"dockerImageVersionId":30804,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
