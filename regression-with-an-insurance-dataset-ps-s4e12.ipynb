{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84896,"databundleVersionId":10305135,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup The Project ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport time\nimport psutil\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nimport warnings\n\nRANDOM_STATE = 42\nwarnings.filterwarnings('ignore')\nnp.random.seed(RANDOM_STATE)\n\nprint(\"Setup completed successfully\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:37:25.088931Z","iopub.execute_input":"2024-12-19T21:37:25.089399Z","iopub.status.idle":"2024-12-19T21:37:25.099207Z","shell.execute_reply.started":"2024-12-19T21:37:25.089361Z","shell.execute_reply":"2024-12-19T21:37:25.097300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input/playground-series-s4e12'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:37:25.513962Z","iopub.execute_input":"2024-12-19T21:37:25.514366Z","iopub.status.idle":"2024-12-19T21:37:25.521858Z","shell.execute_reply.started":"2024-12-19T21:37:25.514332Z","shell.execute_reply":"2024-12-19T21:37:25.520610Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Pipline Monitor\r\n","metadata":{}},{"cell_type":"code","source":"class PipelineMonitor:\n    def __init__(self):\n        self.start_time = time.time()\n        self.checkpoints = {}\n\n    def checkpoint(self, name: str) -> None:\n        \"\"\"Record time taken since last checkpoint\"\"\"\n        current_time = time.time()\n        time_taken = current_time - self.start_time\n        self.checkpoints[name] = time_taken\n        self.start_time = current_time\n\n    def get_memory_usage(self) -> float:\n        \"\"\"Get current memory usage in MB\"\"\"\n        process = psutil.Process()\n        return process.memory_info().rss / 1024 / 1024\n\n# Initialize pipeline monitor\nmonitor = PipelineMonitor()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:37:26.969469Z","iopub.execute_input":"2024-12-19T21:37:26.969853Z","iopub.status.idle":"2024-12-19T21:37:26.978232Z","shell.execute_reply.started":"2024-12-19T21:37:26.969823Z","shell.execute_reply":"2024-12-19T21:37:26.976632Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Collecting Data","metadata":{}},{"cell_type":"code","source":"print(\"Loading data...\")\ndata_dir = Path(\"/kaggle/input/playground-series-s4e12\")\ntrain_path = data_dir / \"train.csv\"\ntest_path = data_dir / \"test.csv\"\n\n\ntry:\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    print(\"Data loaded successfully!\")\nexcept FileNotFoundError as e:\n    print(f\"Error: Data files not found. Please ensure the data files are in the 'data' directory.\\nError: {e}\")\n    raise\n\nmonitor.checkpoint(\"Collecting Data Complete\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:37:30.931551Z","iopub.execute_input":"2024-12-19T21:37:30.932045Z","iopub.status.idle":"2024-12-19T21:37:41.401910Z","shell.execute_reply.started":"2024-12-19T21:37:30.932007Z","shell.execute_reply":"2024-12-19T21:37:41.400388Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"markdown","source":"### Basic Information","metadata":{}},{"cell_type":"code","source":"print(\"\\n1. Basic Information:\")\nprint(\"-\" * 50)\nprint(\"\\nDataset Shape:\", train_df.shape)\nprint(\"\\nFeature Types:\")\nprint(train_df.dtypes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:37:41.404186Z","iopub.execute_input":"2024-12-19T21:37:41.404696Z","iopub.status.idle":"2024-12-19T21:37:41.412391Z","shell.execute_reply.started":"2024-12-19T21:37:41.404642Z","shell.execute_reply":"2024-12-19T21:37:41.411353Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Missing Values Analysis","metadata":{}},{"cell_type":"code","source":"print(\"\\n2. Missing Values Analysis:\")\nprint(\"-\" * 50)\nmissing = train_df.isnull().sum()\nmissing_pct = (missing / len(train_df)) * 100\nmissing_info = pd.DataFrame({\n    'Missing Values': missing,\n    'Percentage': missing_pct\n})\n\nprint(missing_info[missing_info['Missing Values'] > 0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:37:41.413828Z","iopub.execute_input":"2024-12-19T21:37:41.414149Z","iopub.status.idle":"2024-12-19T21:37:42.102031Z","shell.execute_reply.started":"2024-12-19T21:37:41.414120Z","shell.execute_reply":"2024-12-19T21:37:42.100670Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Numerical Features Analysis","metadata":{}},{"cell_type":"code","source":"print(\"\\n3. Numerical Features Analysis:\")\nprint(\"-\" * 50)\nnumeric_cols = train_df.select_dtypes(include=['int64', 'float64']).columns\nnumeric_summary = train_df[numeric_cols].describe()\nprint(numeric_summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:37:42.104463Z","iopub.execute_input":"2024-12-19T21:37:42.104941Z","iopub.status.idle":"2024-12-19T21:37:42.850458Z","shell.execute_reply.started":"2024-12-19T21:37:42.104891Z","shell.execute_reply":"2024-12-19T21:37:42.849209Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Categorical Features Analysis","metadata":{}},{"cell_type":"code","source":"print(\"\\n4. Categorical Features Analysis:\")\nprint(\"-\" * 50)\ncategorical_cols = train_df.select_dtypes(include=['object']).columns\nfor col in categorical_cols:\n    print(f\"\\nUnique values in {col}:\", train_df[col].nunique())\n    print(\"\\nTop 5 categories:\")\n    print(train_df[col].value_counts().head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:37:42.851822Z","iopub.execute_input":"2024-12-19T21:37:42.852221Z","iopub.status.idle":"2024-12-19T21:37:45.442535Z","shell.execute_reply.started":"2024-12-19T21:37:42.852184Z","shell.execute_reply":"2024-12-19T21:37:45.440987Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizations","metadata":{}},{"cell_type":"code","source":"# Distribution plots for numerical features\nplt.figure(figsize=(15, 5 * ((len(numeric_cols) + 2) // 3)))\nfor i, col in enumerate(numeric_cols, 1):\n    plt.subplot((len(numeric_cols) + 2) // 3, 3, i)\n    sns.histplot(train_df[col], kde=True)\n    plt.title(f'Distribution of {col}')\n    plt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\nfor i, col in enumerate(numeric_cols, 1):\n    plt.subplot((len(numeric_cols) + 2) // 3, 3, i)\n    sns.boxplot(y=df[col])\n    plt.title(f'Box Plot of {col}')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:38:05.344346Z","iopub.execute_input":"2024-12-19T21:38:05.345385Z","iopub.status.idle":"2024-12-19T21:38:59.116018Z","shell.execute_reply.started":"2024-12-19T21:38:05.345340Z","shell.execute_reply":"2024-12-19T21:38:59.114960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Correlation analysis\nplt.figure(figsize=(12, 8))\ncorrelation_matrix = train_df[numeric_cols].corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Correlation Matrix of Numerical Features')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:40:41.415217Z","iopub.execute_input":"2024-12-19T21:40:41.415732Z","iopub.status.idle":"2024-12-19T21:40:42.545725Z","shell.execute_reply.started":"2024-12-19T21:40:41.415691Z","shell.execute_reply":"2024-12-19T21:40:42.544514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 5 * ((len(numeric_cols) + 2) // 3)))\nfor i, col in enumerate(numeric_cols, 1):\n    plt.subplot((len(numeric_cols) + 2) // 3, 3, i)\n    sns.boxplot(y=train_df[col])\n    plt.title(f'Box Plot of {col}')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:46:04.014944Z","iopub.execute_input":"2024-12-19T21:46:04.015365Z","iopub.status.idle":"2024-12-19T21:46:06.341841Z","shell.execute_reply.started":"2024-12-19T21:46:04.015327Z","shell.execute_reply":"2024-12-19T21:46:06.340745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Target variable analysis\ntarget_col = 'Premium Amount'\n\nif target_col in train_df.columns:\n    plt.figure(figsize=(12, 5))\n\n    # Target distribution\n    plt.subplot(1, 2, 1)\n    sns.histplot(train_df[target_col], kde=True)\n    plt.title('Distribution of Premium Amount')\n\n    # Log-transformed target distribution\n    plt.subplot(1, 2, 2)\n    sns.histplot(np.log1p(train_df[target_col]), kde=True)\n    plt.title('Distribution of Log-transformed Premium Amount')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Relationship between target and numerical features\n    plt.figure(figsize=(15, 5 * ((len(numeric_cols) - 1 + 2) // 3)))\n    for i, col in enumerate([c for c in numeric_cols if c != target_col], 1):\n        plt.subplot((len(numeric_cols) - 1 + 2) // 3, 3, i)\n        plt.scatter(train_df[col], train_df[target_col], alpha=0.5)\n        plt.xlabel(col)\n        plt.ylabel(target_col)\n        plt.title(f'{col} vs {target_col}')\n    plt.tight_layout()\n    plt.show()\n\nmonitor.checkpoint(\"EDA Complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:43:40.515734Z","iopub.execute_input":"2024-12-19T21:43:40.516852Z","iopub.status.idle":"2024-12-19T21:44:21.061764Z","shell.execute_reply.started":"2024-12-19T21:43:40.516804Z","shell.execute_reply":"2024-12-19T21:44:21.060717Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Validation and Cleaning","metadata":{}},{"cell_type":"code","source":"def validate_data(df: pd.DataFrame) -> Tuple[bool, Dict[str, bool]]:\n    checks = {\n        'missing_values': df.isnull().sum().sum() == 0,\n        'negative_values': (df.select_dtypes(include=['int64', 'float64']) >= 0).all().all(),\n        'duplicates': df.duplicated().sum() == 0,\n    }\n    return all(checks.values()), checks\n\ndef preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Comprehensive data preprocessing pipeline\n\n    Parameters:\n        df: Input DataFrame\n\n    Returns:\n        Preprocessed DataFrame\n    \"\"\"\n    df = df.copy()\n\n    # Handle missing values\n    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n    categorical_cols = df.select_dtypes(include=['object']).columns\n\n    numeric_imputer = SimpleImputer(strategy='median')\n    categorical_imputer = SimpleImputer(strategy='most_frequent')\n\n    df[numeric_cols] = numeric_imputer.fit_transform(df[numeric_cols])\n    df[categorical_cols] = categorical_imputer.fit_transform(df[categorical_cols])\n\n    # Handle outliers using IQR method\n    for col in numeric_cols:\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        df[col] = df[col].clip(lower=Q1 - 1.5*IQR, upper=Q3 + 1.5*IQR)\n\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:47:24.420813Z","iopub.execute_input":"2024-12-19T21:47:24.421206Z","iopub.status.idle":"2024-12-19T21:47:24.430910Z","shell.execute_reply.started":"2024-12-19T21:47:24.421174Z","shell.execute_reply":"2024-12-19T21:47:24.429691Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"class FeatureEngineer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.label_encoders = {}\n        self.scaler = RobustScaler()\n        self.standard_scaler = StandardScaler()\n        self.feature_names = None\n        self.numeric_features = None\n        self.categorical_features = None\n\n    def standardize_column_names(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Standardizes column names by replacing spaces with underscores and converting to lowercase.\"\"\"\n        df.columns = df.columns.str.replace(' ', '_').str.replace('/', '_').str.lower()\n        return df\n\n    def transform_dates(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Extracts date-related features and drops the original date column.\"\"\"\n        df = df.copy()\n        if 'policy_start_date' in df.columns:\n            df['policy_start_date'] = pd.to_datetime(df['policy_start_date'], errors='coerce')\n            df['policy_start_year'] = df['policy_start_date'].dt.year\n            df['policy_start_month'] = df['policy_start_date'].dt.month\n            df['policy_start_quarter'] = df['policy_start_date'].dt.quarter\n            df.drop('policy_start_date', axis=1, inplace=True)\n        return df\n\n    def create_interactions(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Generates basic and advanced interaction features.\"\"\"\n        df = df.copy()\n        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n\n        # Store numeric features for later use\n        self.numeric_features = [col for col in numeric_cols if col not in ['id', 'premium_amount']]\n\n        # Basic ratio features\n        if 'annual_income' in df.columns and 'age' in df.columns:\n            df['age_income_ratio'] = df['age'] / df['annual_income'].clip(lower=1)\n\n        if 'annual_income' in df.columns and 'number_of_dependents' in df.columns:\n            df['income_per_dependent'] = df['annual_income'] / (df['number_of_dependents'].clip(lower=0) + 1)\n\n        if 'health_score' in df.columns and 'age' in df.columns:\n            df['health_age_ratio'] = df['health_score'] / df['age'].clip(lower=1)\n\n        # Risk-based features\n        if all(col in df.columns for col in ['health_score', 'credit_score', 'age']):\n            df['risk_score'] = (df['health_score'] * df['credit_score']) / df['age'].clip(lower=1)\n\n        if all(col in df.columns for col in ['previous_claims', 'insurance_duration']):\n            df['claims_duration_ratio'] = df['previous_claims'] / df['insurance_duration'].clip(lower=1)\n\n        # Demographic features\n        if 'number_of_dependents' in df.columns and 'annual_income' in df.columns:\n            df['dependent_burden'] = df['number_of_dependents'] / df['annual_income'].clip(lower=1)\n\n        if 'age' in df.columns and 'health_score' in df.columns:\n            df['age_risk_factor'] = df['age'] * (1 / df['health_score'].clip(lower=1))\n\n        # Insurance history features\n        if 'previous_claims' in df.columns and 'insurance_duration' in df.columns:\n            df['claims_frequency'] = df['previous_claims'] / df['insurance_duration'].clip(lower=1)\n\n        # Polynomial features for key numeric columns\n        for col in ['age', 'health_score', 'credit_score']:\n            if col in df.columns:\n                df[f'{col}_squared'] = df[col] ** 2\n\n        return df\n\n    def encode_categoricals(self, df: pd.DataFrame, is_training: bool = True) -> pd.DataFrame:\n        \"\"\"Encodes categorical features with Label Encoding.\"\"\"\n        df = df.copy()\n        categorical_cols = df.select_dtypes(include=['object']).columns\n\n        # Store categorical features for later use\n        if is_training:\n            self.categorical_features = list(categorical_cols)\n\n        for col in categorical_cols:\n            df[col] = df[col].fillna('Unknown')\n            if is_training:\n                self.label_encoders[col] = LabelEncoder()\n                df[col] = self.label_encoders[col].fit_transform(df[col])\n            else:\n                if col in self.label_encoders:\n                    # Get unique categories and create a mapping dictionary\n                    known_categories = set(self.label_encoders[col].classes_)\n                    unique_vals = df[col].unique()\n                    val_map = {val: (\n                        self.label_encoders[col].transform([val])[0] if val in known_categories else -1\n                    ) for val in unique_vals}\n                    # Use replace which is much faster than map\n                    df[col] = df[col].replace(val_map)\n        return df\n\n    def scale_features(self, df: pd.DataFrame, is_training: bool = True) -> pd.DataFrame:\n        \"\"\"Scales numerical features using RobustScaler.\"\"\"\n        df = df.copy()\n        if not self.numeric_features:\n            self.numeric_features = df.select_dtypes(include=['int64', 'float64']).columns\n            self.numeric_features = [col for col in self.numeric_features if col not in ['id', 'premium_amount']]\n\n        if self.numeric_features:\n            if is_training:\n                df[self.numeric_features] = self.scaler.fit_transform(df[self.numeric_features].fillna(0))\n            else:\n                df[self.numeric_features] = self.scaler.transform(df[self.numeric_features].fillna(0))\n        return df\n\n    def transform(self, df: pd.DataFrame, is_training: bool = True) -> pd.DataFrame:\n        \"\"\"Applies all transformations in sequence.\"\"\"\n        df = self.standardize_column_names(df)\n        df = self.transform_dates(df)\n        df = self.create_interactions(df)\n        df = self.encode_categoricals(df, is_training)\n        df = self.scale_features(df, is_training)\n\n        # Store feature names after all transformations\n        self.feature_names = list(df.columns)\n        return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:47:47.569625Z","iopub.execute_input":"2024-12-19T21:47:47.570118Z","iopub.status.idle":"2024-12-19T21:47:47.598010Z","shell.execute_reply.started":"2024-12-19T21:47:47.570080Z","shell.execute_reply":"2024-12-19T21:47:47.596557Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"class ModelTrainer:\n    def __init__(self, random_state=42):\n        self.random_state = random_state\n        self.models = {}\n        self.y_true_all_folds = []\n        self.y_pred_all_folds = []\n        self.best_model = None\n        self.feature_importance = None\n\n    def train_xgboost(self, X_train, y_train, X_val=None, y_val=None):\n        \"\"\"Train XGBoost model with early stopping if validation data is provided.\"\"\"\n        params = {\n            'objective': 'reg:squarederror',\n            'eval_metric': 'rmse',\n            'max_depth': 8,\n            'learning_rate': 0.05,\n            'n_estimators': 1000,\n            'min_child_weight': 3,\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'random_state': self.random_state,\n            'early_stopping_rounds': 50,\n            'verbose': False\n        }\n\n        model = xgb.XGBRegressor(**params)\n\n        if X_val is not None and y_val is not None:\n            model.fit(\n                X_train, y_train,\n                eval_set=[(X_train, y_train), (X_val, y_val)]\n            )\n        else:\n            model.fit(X_train, y_train)\n\n        return model\n\n    def train_lightgbm(self, X_train, y_train, X_val=None, y_val=None):\n        \"\"\"Train LightGBM model with early stopping if validation data is provided.\"\"\"\n        params = {\n            'objective': 'regression',\n            'metric': 'rmse',\n            'num_leaves': 31,\n            'learning_rate': 0.05,\n            'feature_fraction': 0.8,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'random_state': self.random_state,\n            'n_estimators': 1000,\n            'verbose': -1\n        }\n\n        model = lgb.LGBMRegressor(**params)\n\n        if X_val is not None and y_val is not None:\n            callbacks = [lgb.early_stopping(stopping_rounds=50)]\n            model.fit(\n                X_train, y_train,\n                eval_set=[(X_val, y_val)],\n                callbacks=callbacks\n            )\n        else:\n            model.fit(X_train, y_train)\n\n        return model\n\n    def train_catboost(self, X_train, y_train, X_val=None, y_val=None):\n        \"\"\"Train CatBoost model with early stopping if validation data is provided.\"\"\"\n        params = {\n            'loss_function': 'RMSE',\n            'eval_metric': 'RMSE',\n            'learning_rate': 0.05,\n            'depth': 6,\n            'iterations': 1000,\n            'random_seed': self.random_state,\n            'verbose': False\n        }\n\n        model = cb.CatBoostRegressor(**params)\n\n        if X_val is not None and y_val is not None:\n            model.fit(\n                X_train, y_train,\n                eval_set=(X_val, y_val),\n                early_stopping_rounds=50,\n                verbose=False\n            )\n        else:\n            model.fit(X_train, y_train)\n\n        return model\n\n    def train_with_kfold(self, X, y, n_splits=5):\n        \"\"\"Train models using K-Fold cross validation.\"\"\"\n        print(\"\\nStarting K-Fold Cross Validation Training...\")\n        kf = KFold(n_splits=n_splits, shuffle=True, random_state=self.random_state)\n\n        fold_scores = {\n            'xgboost': [],\n            'lightgbm': [],\n            'catboost': []\n        }\n\n        model_predictions = {\n            'xgboost': [],\n            'lightgbm': [],\n            'catboost': []\n        }\n\n        for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n            print(f\"\\nTraining Fold {fold}/{n_splits}\")\n\n            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n            # Train each model\n            self.models[f'xgboost_fold_{fold}'] = self.train_xgboost(X_train, y_train, X_val, y_val)\n            self.models[f'lightgbm_fold_{fold}'] = self.train_lightgbm(X_train, y_train, X_val, y_val)\n            self.models[f'catboost_fold_{fold}'] = self.train_catboost(X_train, y_train, X_val, y_val)\n\n            # Make predictions\n            y_pred_xgb = self.models[f'xgboost_fold_{fold}'].predict(X_val)\n            y_pred_lgb = self.models[f'lightgbm_fold_{fold}'].predict(X_val)\n            y_pred_cb = self.models[f'catboost_fold_{fold}'].predict(X_val)\n\n            # Store predictions\n            model_predictions['xgboost'].append((fold, y_pred_xgb))\n            model_predictions['lightgbm'].append((fold, y_pred_lgb))\n            model_predictions['catboost'].append((fold, y_pred_cb))\n\n            # Calculate RMSLE for each model\n            fold_scores['xgboost'].append(np.sqrt(mean_squared_log_error(y_val, y_pred_xgb)))\n            fold_scores['lightgbm'].append(np.sqrt(mean_squared_log_error(y_val, y_pred_lgb)))\n            fold_scores['catboost'].append(np.sqrt(mean_squared_log_error(y_val, y_pred_cb)))\n\n            # Store true values for this fold\n            self.y_true_all_folds.extend(y_val)\n\n            # Store ensemble predictions for this fold\n            ensemble_pred = (y_pred_xgb + y_pred_lgb + y_pred_cb) / 3\n            self.y_pred_all_folds.extend(ensemble_pred)\n\n        # Print average scores\n        print(\"\\nAverage RMSLE scores across folds:\")\n        avg_scores = {}\n        for model_name, scores in fold_scores.items():\n            avg_score = np.mean(scores)\n            std_score = np.std(scores)\n            print(f\"{model_name}: {avg_score:.4f} (+/- {std_score:.4f})\")\n            avg_scores[model_name] = avg_score\n\n        # Select best model type based on average performance\n        best_model_type = min(avg_scores.items(), key=lambda x: x[1])[0]\n\n        # Find the best fold for the best model type\n        best_fold_idx = np.argmin(fold_scores[best_model_type])\n        best_fold = best_fold_idx + 1\n\n        # Set the best model\n        self.best_model = self.models[f'{best_model_type}_fold_{best_fold}']\n\n        # Get feature importance from the best model\n        if hasattr(self.best_model, 'feature_importances_'):\n            self.feature_importance = self.best_model.feature_importances_\n\n        print(f\"\\nBest Model: {best_model_type} from fold {best_fold}\")\n        return self.models\n\n    def get_y_true_and_pred(self):\n        \"\"\"Get true and predicted values from all folds.\"\"\"\n        return np.array(self.y_true_all_folds), np.array(self.y_pred_all_folds)\n\n    def evaluate_models(self):\n        \"\"\"Evaluate all trained models using RMSLE metric.\"\"\"\n        if not self.y_true_all_folds or not self.y_pred_all_folds:\n            print(\"No predictions available. Please train the models first.\")\n            return None\n\n        y_true = np.array(self.y_true_all_folds)\n        y_pred = np.array(self.y_pred_all_folds)\n\n        metrics = {\n            'RMSLE': np.sqrt(mean_squared_log_error(y_true, y_pred)),\n            'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n            'MAE': mean_absolute_error(y_true, y_pred),\n            'R2': r2_score(y_true, y_pred)\n        }\n\n        print(\"\\nModel Evaluation Metrics:\")\n        for metric_name, value in metrics.items():\n            print(f\"{metric_name}: {value:.4f}\")\n\n        return metrics\n\n    def predict(self, X):\n        \"\"\"Make predictions using the best model.\"\"\"\n        if self.best_model is None:\n            raise ValueError(\"No model available. Please train the models first.\")\n        return self.best_model.predict(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:48:14.201144Z","iopub.execute_input":"2024-12-19T21:48:14.201614Z","iopub.status.idle":"2024-12-19T21:48:14.231998Z","shell.execute_reply.started":"2024-12-19T21:48:14.201554Z","shell.execute_reply":"2024-12-19T21:48:14.230576Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training and Evaluation","metadata":{}},{"cell_type":"code","source":"# Start the training pipeline\nprint(\"\\nStarting main training pipeline...\")\n\n# Initialize the feature engineer\nfe = FeatureEngineer()\n\n# Prepare train and test data\nprint(\"Preparing train and test data...\")\nX_train = train_df.drop(['Premium Amount', 'id'], axis=1)\ny_train = np.log1p(train_df['Premium Amount'])  # Log transform target\nX_test = test_df.drop(['id'], axis=1)\n\n# Apply feature transformations\nprint(\"Transforming features...\")\nX_train_transformed = fe.transform(X_train, is_training=True)\nX_test_transformed = fe.transform(X_test, is_training=False)\nprint(\"Features prepared!\")\n\n# Initialize the model trainer\ntrainer = ModelTrainer(random_state=42)\n\n# Train models with K-Fold Cross-Validation\nprint(\"Training models with K-Fold Cross-Validation...\")\ntry:\n    models = trainer.train_with_kfold(X_train_transformed, y_train)\n    print(\"Training completed!\")\n\n    # Evaluate models\n    print(\"Evaluating models...\")\n    evaluations = trainer.evaluate_models()\n    print(f\"Model Evaluations: {evaluations}\")\n\n    # Make predictions with the best model\n    print(\"Making predictions with the best model...\")\n    if trainer.best_model is not None:\n        test_predictions = trainer.predict(X_test_transformed)\n\n        # Transform predictions back from log space\n        final_predictions = np.expm1(test_predictions)\n\n        # Prepare submission\n        submission = pd.DataFrame({\n            'id': test_df['id'],\n            'Premium Amount': final_predictions\n        })\n\n        # Save predictions\n        output_path = 'submission.csv'\n        submission.to_csv(output_path, index=False)\n        print(f\"\\nPredictions saved to {output_path}\")\n\n    else:\n        print(\"Error: Best model not selected properly during training.\")\nexcept Exception as e:\n    print(f\"Error during model training/prediction: {str(e)}\")\n\nmonitor.checkpoint(\"Analysis Complete\")\n\n# Print final execution summary\nprint(\"\\nExecution Summary:\")\nfor checkpoint, time_taken in monitor.checkpoints.items():\n    print(f\"{checkpoint}: {time_taken:.2f}s\")\nprint(f\"Final Memory Usage: {monitor.get_memory_usage():.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T21:48:46.079527Z","iopub.execute_input":"2024-12-19T21:48:46.080541Z","iopub.status.idle":"2024-12-19T22:06:26.473838Z","shell.execute_reply.started":"2024-12-19T21:48:46.080495Z","shell.execute_reply":"2024-12-19T22:06:26.472629Z"}},"outputs":[],"execution_count":null}]}